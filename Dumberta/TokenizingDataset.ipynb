{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c3faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import RobertaTokenizerFast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27798e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024\n",
    "test_file = \"/home/johnmf4/ProteinTransformersResearch/Datasets/UniRef90_Data/uniref90_test.txt\"\n",
    "train_file = \"/home/johnmf4/ProteinTransformersResearch/Datasets/UniRef90_Data/shrunked_train.txt\"\n",
    "validate_file = \"/home/johnmf4/ProteinTransformersResearch/Datasets/UniRef90_Data/uniref90_valid.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e880de2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0a12617fa55b7f56\n",
      "Reusing dataset text (/home/johnmf4/.cache/huggingface/datasets/text/default-0a12617fa55b7f56/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "Using custom data configuration default-a7d584368eac2e27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/johnmf4/.cache/huggingface/datasets/text/default-a7d584368eac2e27/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08399da02a240d0a78d9936c5a869ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/johnmf4/.cache/huggingface/datasets/text/default-a7d584368eac2e27/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6babee5646b94fd1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/johnmf4/.cache/huggingface/datasets/text/default-6babee5646b94fd1/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a879c6ffb3564a91baee09d78211e3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/johnmf4/.cache/huggingface/datasets/text/default-6babee5646b94fd1/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('text', data_files={'train': train_file})\n",
    "test_dataset = load_dataset('text', data_files={'test': test_file})\n",
    "validate_dataset = load_dataset('text', data_files={'train': validate_file})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003f8b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"../models/Dumberta\", model_max_length=max_length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db712a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f82ad84a28e4bb7ba0ec64f06fac711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb90c72331340b59fbe52e5d5e0f20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138a309557d841da9ba381818b74dc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(lambda examples: tokenizer(examples['text'], truncation=True, max_length=max_length), batched=True, batch_size=100000, writer_batch_size=100000)\n",
    "tokenized_test_dataset = test_dataset.map(lambda examples: tokenizer(examples['text'], truncation=True, max_length=max_length), batched=True, batch_size=100000, writer_batch_size=100000)\n",
    "tokenized_validation_dataset = validate_dataset.map(lambda examples: tokenizer(examples['text'], truncation=True, max_length=max_length), batched=True, batch_size=100000, writer_batch_size=100000)\n",
    "\n",
    "tokenized_test_dataset.save_to_disk(\"../Datasets/tokenized_test_dataset_dumberta\")\n",
    "tokenized_train_dataset.save_to_disk(\"../Datasets/tokenized_train_dataset_dumberta\")\n",
    "tokenized_validation_dataset.save_to_disk(\"../Datasets/tokenized_validation_dataset_dumberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55529e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ProteinTransformers3]",
   "language": "python",
   "name": "conda-env-.conda-ProteinTransformers3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
